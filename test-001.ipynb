{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34acf43e-a76c-441b-bb3e-313f0e3a34a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=12, micro=12, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439b5e61-092e-48c7-9f6d-1e5fb04a31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d815360d-f456-4201-82f9-ede0e1cdfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    # .master('dev-java.home.lan')\n",
    "    .master('local[4]')\n",
    "    .appName('mySpark')\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e87d9f-f729-4e6f-a8f9-8f897eb389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36fc608-0ea9-4c78-8d5e-eefa8b036afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56791dc-917e-4caf-8a0a-46cf71b2094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_1: string (nullable = true)\n",
      " |-- col_2: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dt_ts: timestamp (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      " |-- dt1: string (nullable = true)\n",
      "\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "|col_1|col_2| id|              dt_ts|        dt|                dt1|\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "|   s1| s2-1|  1|2025-12-25 13:00:56|      NULL|25-12-2025T13:01:56|\n",
      "|   s2| s2-2|  2|2025-11-30 01:19:45|      NULL|               NULL|\n",
      "|   s3| s2-3|  3|2025-12-26 00:00:00|2025-11-29|25-11-2021 14:10:56|\n",
      "|   s4| s2-4|  4|               NULL|      NULL|               NULL|\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- col_1: string (nullable = true)\n",
      " |-- col_2: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dt_ts: timestamp (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      " |-- dt1: string (nullable = true)\n",
      " |-- dtn1: string (nullable = true)\n",
      "\n",
      "+-----+-----+---+-------------------+----------+-------------------+------------------+\n",
      "|col_1|col_2|id |dt_ts              |dt        |dt1                |dtn1              |\n",
      "+-----+-----+---+-------------------+----------+-------------------+------------------+\n",
      "|s1   |s2-1 |1  |2025-12-25 13:00:56|NULL      |25-12-2025T13:01:56|25--12--2025 13:01|\n",
      "|s2   |s2-2 |2  |2025-11-30 01:19:45|NULL      |NULL               |NULL              |\n",
      "|s3   |s2-3 |3  |2025-12-26 00:00:00|2025-11-29|25-11-2021 14:10:56|25--11--2021 14:10|\n",
      "|s4   |s2-4 |4  |NULL               |NULL      |NULL               |NULL              |\n",
      "+-----+-----+---+-------------------+----------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType\n",
    "\n",
    "sm = StructType(fields = [\n",
    "    StructField(\"col_1\", StringType()),\n",
    "    StructField(\"col_2\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"dt_ts\", TimestampType()),\n",
    "    StructField(\"dt\", DateType()),\n",
    "    StructField(\"dt1\", StringType())\n",
    "])\n",
    "df = spark.read.csv(\"1.csv\", schema=sm, sep=';')\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "#df_ = df.withColumn(\"dtn\", F.to_timestamp(\"dt1\", when(\"dd-MM-yyyy'T'HH:mm:ss\", df[\"dt1\"].contains(\"T\"))))\n",
    "# df_ = df.withColumn(\n",
    "#     \"dtn1\",\n",
    "#     F.when(df[\"dt1\"].contains(\"T\"), F.to_timestamp(\"dt1\", \"dd-MM-yyyy'T'HH:mm:ss\"))\n",
    "#     .otherwise(F.to_timestamp(\"dt1\", \"dd-MM-yyyy HH:mm:ss\"))\n",
    "# )\n",
    "df_ = df.withColumn(\n",
    "    \"dtn1\",\n",
    "    F.date_format(\n",
    "    F.when(df.dt1.contains(\"T\"), F.to_timestamp(\"dt1\", \"dd-MM-yyyy'T'HH:mm:ss\"))\n",
    "    .otherwise(F.to_timestamp(\"dt1\", \"dd-MM-yyyy HH:mm:ss\"))\n",
    "        , \"dd--MM--yyyy HH:mm\"\n",
    "    )\n",
    ")\n",
    "#df_ = df.withColumn(\"dtn\", F.to_timestamp(\"dt1\", \"dd-MM-yyyy HH:mm:ss\"))\n",
    "df_.printSchema()\n",
    "df_.show(5, truncate=False)\n",
    "#df.select('*', F.regexp_replace('dt1', r'[^\\d:-]', ' ')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9270c4f4-6e1c-4a3e-b5c9-0d414fb7c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-------------------+----------+-------------------+------------------+\n",
      "|col_1|col_2|id |dt_ts              |dt        |dt1                |dtn1              |\n",
      "+-----+-----+---+-------------------+----------+-------------------+------------------+\n",
      "|s1   |s2-1 |1  |2025-12-25 13:00:56|NULL      |25-12-2025T13:01:56|25--12--2025 13:01|\n",
      "|s2   |s2-2 |2  |2025-11-30 01:19:45|NULL      |NULL               |NULL              |\n",
      "|s3   |s2-3 |3  |2025-12-26 00:00:00|2025-11-29|25-11-2021 14:10:56|25--11--2021 14:10|\n",
      "|s4   |s2-4 |4  |NULL               |NULL      |NULL               |NULL              |\n",
      "+-----+-----+---+-------------------+----------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ = df.withColumn(\n",
    "    \"dtn1\",\n",
    "    F.date_format(\n",
    "        F.when(df.dt1.contains(\"T\"), F.to_timestamp(\"dt1\", \"dd-MM-yyyy'T'HH:mm:ss\"))\n",
    "                .otherwise(F.to_timestamp(\"dt1\", \"dd-MM-yyyy HH:mm:ss\"))\n",
    "        , \"dd--MM--yyyy HH:mm\"\n",
    "    )\n",
    ")\n",
    "#df_ = df.withColumn(\"dtn\", F.to_timestamp(\"dt1\", \"dd-MM-yyyy HH:mm:ss\"))\n",
    "# df_.printSchema()\n",
    "df_.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ed9c82b-4ef7-409b-8bd8-d9f624155c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_1: string (nullable = true)\n",
      " |-- col_2: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dt_ts: timestamp (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      " |-- dt1: string (nullable = true)\n",
      "\n",
      "+-----+-----+----+-----+----+----+\n",
      "|col_1|col_2|  id|dt_ts|  dt| dt1|\n",
      "+-----+-----+----+-----+----+----+\n",
      "| NULL| NULL|NULL| NULL|NULL|NULL|\n",
      "+-----+-----+----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.json(\"2.json\", sm, multiLine=True)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "085f24f9-8cf0-495f-bb82-5dd07130beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_1: string (nullable = true)\n",
      " |-- col_2: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dt_ts: timestamp (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      " |-- dt1: string (nullable = true)\n",
      "\n",
      "+-----+-----+----+-----+----+----+\n",
      "|col_1|col_2|  id|dt_ts|  dt| dt1|\n",
      "+-----+-----+----+-----+----+----+\n",
      "| s1-1| s1-2|NULL| NULL|NULL|NULL|\n",
      "| s2-1| s2-2|NULL| NULL|NULL|NULL|\n",
      "+-----+-----+----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.json(\"3.json\", sm, multiLine=True)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5dde108-0785-4c52-ac43-472597eddb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "|col_1|col_2| id|              dt_ts|        dt|                dt1|\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "|   s1| s2-1|  1|2025-12-25 13:00:56|      NULL|25-12-2025T13:01:56|\n",
      "|   s2| s2-2|  2|2025-11-30 01:19:45|      NULL|               NULL|\n",
      "|   s3| s2-3|  3|2025-12-26 00:00:00|2025-11-29|25-11-2021 14:10:56|\n",
      "|   s4| s2-4|  4|               NULL|      NULL|               NULL|\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "|col_1|col_2| id|              dt_ts|        dt|                dt1|\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "|   s3| s2-3|  3|2025-12-26 00:00:00|2025-11-29|25-11-2021 14:10:56|\n",
      "+-----+-----+---+-------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"view1\")\n",
    "df5 = spark.sql(\"select * from view1\")\n",
    "df5.show()\n",
    "df4 = spark.sql(\"select * from view1 where dt_ts >= dt\")\n",
    "df4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10777e83-7a0a-4d7d-af2b-ec66dda78dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
